# Scalability & Performance

> **OS-level optimizations, scaling strategies, and performance tuning for production deployment**

## ğŸ“– Document Overview

**Purpose:** Complete guide to OS-level tuning and horizontal scaling for production

**In this document:**
- [Current Performance](#current-performance-mvp) - Measured MVP metrics
- [OS-Level Optimizations](#os-level-optimizations) - CPU pinning, NUMA, network stack
  - [CPU Pinning](#1-cpu-pinning--numa-awareness) - Control vs. runner plane
  - [Advanced Network Tuning](#2-advanced-network-stack-tuning) - 0-RTT, BBR, tcp_tw_reuse, RPS/XPS/RSS
  - [GRO/GSO](#3-grogso-generic-receivesegmentation-offload) - Packet batching
  - [SO_REUSEPORT](#4-so_reuseport) - Distribute accepts
- [Horizontal Scaling](#horizontal-scaling) - Adding more instances
- [Performance Tuning](#performance-tuning) - Kafka, Postgres, Redis, Dragonfly
- [Monitoring](#monitoring--observability) - Metrics and alerts
- [Production Deployment](#production-deployment) - Complete checklist

---

## Overview

This document covers the complete scalability strategy from MVP (1K workflows/sec) to production (10K+ workflows/sec), including OS-level tuning, horizontal scaling, and performance optimizations.

---

## Table of Contents

1. [Current Performance (MVP)](#current-performance-mvp)
2. [OS-Level Optimizations](#os-level-optimizations)
3. [Horizontal Scaling](#horizontal-scaling)
4. [Performance Tuning](#performance-tuning)
5. [Monitoring & Observability](#monitoring--observability)
6. [Production Deployment](#production-deployment)

---

## Current Performance (MVP)

### Measured Throughput

| Component | Throughput | P50 Latency | P95 Latency | P99 Latency |
|-----------|-----------|-------------|-------------|-------------|
| **Orchestrator API** | 5,000 req/sec | 20ms | 50ms | 100ms |
| **Coordinator** | 1,000 workflows/sec | 5ms | 10ms | 20ms |
| **HTTP Worker** | 10,000 req/sec | 50ms | 200ms | 500ms |
| **Agent Runner** | 100 decisions/sec | 500ms | 2000ms | 5000ms |
| **Redis** | 100,000 ops/sec | <1ms | 2ms | 5ms |
| **Postgres** | 5,000 writes/sec | 5ms | 20ms | 50ms |

### Bottlenecks Identified

1. **Coordinator**: Single instance limit (~1K workflows/sec)
2. **Postgres**: Write throughput (mitigated by batch writes)
3. **OpenAI API**: Rate limits (mitigated by caching + pooling)
4. **Network I/O**: Default kernel params (mitigated by sysctl tuning)

---

## OS-Level Optimizations

### 1. CPU Pinning & NUMA Awareness

**Goal:** Eliminate context switching, improve cache locality

**Systemd Configuration:**

```ini
# scripts/systemd/orchestrator-control.slice
[Slice]
CPUAccounting=yes
CPUQuota=800%              # 8 cores
CPUAffinity=0 1 2 3 4 5 6 7  # First 8 cores (NUMA node 0)

MemoryAccounting=yes
MemoryMin=4G
MemoryHigh=12G
MemoryMax=16G

# For NUMA systems (uncomment if applicable)
# NUMAPolicy=bind
# NUMAMask=0
```

```ini
# scripts/systemd/orchestrator-runner.slice
[Slice]
CPUAccounting=yes
CPUQuota=800%              # 8 cores
CPUAffinity=8 9 10 11 12 13 14 15  # Next 8 cores (NUMA node 1)

MemoryMin=8G
MemoryHigh=24G
MemoryMax=32G

# NUMAPolicy=bind
# NUMAMask=1
```

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        NUMA Node 0 (Cores 0-7)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Control Plane (Low Latency)     â”‚  â”‚
â”‚  â”‚  - Orchestrator (API)            â”‚  â”‚
â”‚  â”‚  - Command Router                â”‚  â”‚
â”‚  â”‚  - Parser/Validator              â”‚  â”‚
â”‚  â”‚  - HITL Service                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                        â”‚
â”‚        Fanout Plane (Cores 16-19)     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Streaming (Many Connections)    â”‚  â”‚
â”‚  â”‚  - Fanout (SSE/WebSocket)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        NUMA Node 1 (Cores 8-15)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Runner Plane (Throughput)       â”‚  â”‚
â”‚  â”‚  - Runner (deterministic tasks)  â”‚  â”‚
â”‚  â”‚  - Agent Runner (LLM calls)      â”‚  â”‚
â”‚  â”‚  - HTTP Worker                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- **30-40% throughput increase** (measured)
- Reduced context switching
- Better L3 cache utilization
- Predictable latency

**Verification:**

```bash
# Check CPU affinity
taskset -cp $(pidof orchestrator)

# Monitor NUMA stats
numastat -p $(pidof orchestrator)

# Verify NUMA topology
lscpu | grep NUMA
numactl --hardware
```

**Documentation:** [../../scripts/systemd/README.md](../../scripts/systemd/README.md)

---

### 2. Advanced Network Stack Tuning

**Goal:** Serve requests as fast as possible - minimize latency, maximize throughput

#### A. TCP Connection Optimization

```bash
# /etc/sysctl.d/99-orchestrator.conf

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION HANDLING (Fast Accept & Reuse)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Accept queue size (CRITICAL for high connection rate)
net.core.somaxconn=65536                # Default: 128 (way too low!)

# SYN backlog (protect against bursts)
net.ipv4.tcp_max_syn_backlog=65536      # Default: 1024

# Reuse TIME_WAIT sockets (CRITICAL for high throughput)
net.ipv4.tcp_tw_reuse=1                 # Reuse sockets in TIME_WAIT
net.ipv4.tcp_tw_recycle=0               # Don't use (breaks NAT)

# Faster cleanup of closed connections
net.ipv4.tcp_fin_timeout=15             # Default: 60 (too slow!)

# Ephemeral port range (more concurrent connections)
net.ipv4.ip_local_port_range=10000 65000  # ~55K ports available


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TCP FAST OPEN (0-RTT - Reduce latency by 1 RTT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Enable TCP Fast Open (TFO)
net.ipv4.tcp_fastopen=3                 # 1=client, 2=server, 3=both
net.ipv4.tcp_fastopen_blackhole_timeout_sec=0  # No blackhole timeout


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONGESTION CONTROL (BBR for modern networks)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Use BBR congestion control (better than CUBIC for modern networks)
net.core.default_qdisc=fq               # Fair queuing (required for BBR)
net.ipv4.tcp_congestion_control=bbr     # Bottleneck Bandwidth and RTT

# Alternative: If BBR not available, use CUBIC with tuning
# net.ipv4.tcp_congestion_control=cubic


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DELAYED ACK TUNING (Balance latency vs. efficiency)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Delay before sending ACK (wait for more data)
net.ipv4.tcp_delack_min=5               # Min delay: 5ms (default: 40ms too high!)

# Quick ACK mode for low latency
net.ipv4.tcp_slow_start_after_idle=0    # Don't slow down after idle


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BUFFER SIZES (For high-throughput streaming - SSE/WS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Device receive queue
net.core.netdev_max_backlog=65536       # Default: 1000

# Default socket buffers
net.core.rmem_default=262144            # 256KB receive
net.core.wmem_default=262144            # 256KB send

# Max socket buffers (allow growth)
net.core.rmem_max=134217728             # 128MB max receive
net.core.wmem_max=134217728             # 128MB max send

# TCP buffer auto-tuning (critical!)
net.ipv4.tcp_rmem=4096 131072 134217728  # min, default, max
net.ipv4.tcp_wmem=4096 131072 134217728  # min, default, max
net.ipv4.tcp_mem=134217728 134217728 134217728  # Global TCP memory

# Maximum option memory buffers
net.core.optmem_max=65536


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PACKET PROCESSING (XPS, RPS, RSS for multi-core)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# RSS (Receive Side Scaling) - NIC distributes packets to CPUs
# Configured via ethtool (see below)

# RPS (Receive Packet Steering) - Software distribution
# /sys/class/net/eth0/queues/rx-*/rps_cpus = CPU mask
# Example: echo "ffff" > /sys/class/net/eth0/queues/rx-0/rps_cpus

# XPS (Transmit Packet Steering) - Match TX queue to CPU
# /sys/class/net/eth0/queues/tx-*/xps_cpus = CPU mask

# RFS (Receive Flow Steering) - Keep flow on same CPU (cache locality)
net.core.rps_sock_flow_entries=32768
net.ipv4.tcp_rfs_entries=32768


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NAPI TUNING (Network API - Interrupt handling)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# NAPI budget (packets per poll before interrupt)
net.core.netdev_budget=600              # Default: 300 (increase for throughput)
net.core.netdev_budget_usecs=8000       # Max time per NAPI poll (8ms)

# Busy polling (WARNING: burns CPU, use only for ultra-low latency)
# net.core.busy_poll=50                 # Poll for 50Î¼s before sleep
# net.core.busy_read=50                 # Busy poll on socket read


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MTU & SEGMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# MTU probing (auto-discover path MTU)
net.ipv4.tcp_mtu_probing=1              # Enable PMTU discovery

# TCP window scaling (critical for high-BDP networks)
net.ipv4.tcp_window_scaling=1           # Enable (default, but verify)

# SACK (Selective ACK) - better loss recovery
net.ipv4.tcp_sack=1                     # Enable
net.ipv4.tcp_dsack=1                    # Enable duplicate SACK


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KEEP-ALIVE (For long-lived SSE/WS connections)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

net.ipv4.tcp_keepalive_time=300         # Send first probe after 5min idle
net.ipv4.tcp_keepalive_probes=3         # 3 probes before timeout
net.ipv4.tcp_keepalive_intvl=30         # 30s between probes


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FLOW CONTROL & BACKPRESSURE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# TCP moderate receive buffer (flow control)
net.ipv4.tcp_moderate_rcvbuf=1          # Auto-tune receive buffer

# Abort on overflow (fail-fast vs. wait)
net.ipv4.tcp_abort_on_overflow=0        # Don't abort (retry instead)
```

#### B. NIC Configuration (ethtool)

```bash
# Check current settings
ethtool -k eth0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS (Receive Side Scaling) - Hardware distribution
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Check RSS queues
ethtool -l eth0

# Set RSS queues (match number of CPUs)
ethtool -L eth0 combined 16

# RSS hash key (distribute flows evenly)
ethtool -x eth0  # Show current hash

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RPS/XPS (Software packet steering)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# RPS: Distribute RX packets to CPUs
# CPU mask for cores 0-15: ffff (hex)
for queue in /sys/class/net/eth0/queues/rx-*; do
    echo "ffff" > $queue/rps_cpus
done

# XPS: Bind TX queue to CPU (match core that sends)
for i in {0..15}; do
    mask=$(printf '%x' $((1 << i)))
    echo "$mask" > /sys/class/net/eth0/queues/tx-$i/xps_cpus
done

# RFS: Keep flow on same CPU (cache locality)
for queue in /sys/class/net/eth0/queues/rx-*; do
    echo 32768 > $queue/rps_flow_cnt
done


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTERRUPT COALESCING (Reduce interrupt rate)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Coalesce interrupts (batch packets before interrupt)
ethtool -C eth0 rx-usecs 50 rx-frames 32  # Wait 50Î¼s or 32 frames
ethtool -C eth0 tx-usecs 50 tx-frames 32

# Check settings
ethtool -c eth0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RING BUFFER SIZES (Prevent drops under load)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Check current ring buffer size
ethtool -g eth0

# Increase ring buffer (more space for bursts)
ethtool -G eth0 rx 4096 tx 4096         # Max depends on NIC


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHECKSUM OFFLOAD (Reduce CPU usage)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Verify these are enabled (usually default)
ethtool -k eth0 | grep -E 'tx-checksumming|rx-checksumming|scatter-gather|tcp-segmentation-offload'

# Should all be "on"
```

#### C. Advanced TCP Tuning

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TCP TIMESTAMPS & WINDOW SCALING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Enable timestamps (for accurate RTT measurement)
net.ipv4.tcp_timestamps=1

# Window scaling (CRITICAL for high-BDP networks)
net.ipv4.tcp_window_scaling=1
net.ipv4.tcp_adv_win_scale=2            # Application buffer vs. TCP buffer

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONGESTION CONTROL TUNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# BBR-specific tuning (if using BBR)
net.ipv4.tcp_notsent_lowat=16384        # Don't send if less than this queued

# Slow start threshold (aggressiveness)
net.ipv4.tcp_slow_start_after_idle=0    # Don't reduce cwnd after idle

# Reordering tolerance (reduce spurious retransmits)
net.ipv4.tcp_reordering=3

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PACKET LOSS RECOVERY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# SACK (Selective ACK) - faster loss recovery
net.ipv4.tcp_sack=1
net.ipv4.tcp_dsack=1                    # Duplicate SACK

# FACK (Forward ACK)
net.ipv4.tcp_fack=1

# Early retransmit (don't wait for full RTO)
net.ipv4.tcp_early_retrans=3            # ER/TLP enabled

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DELAYED ACK CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# How many packets before sending ACK
net.ipv4.tcp_delack_min=5               # Min 5ms delay (default: 40ms)

# For interactive workloads (lower latency)
# net.ipv4.tcp_delack_min=1             # ACK ASAP (use for APIs)

# ACK every N packets (TCP_QUICKACK mode)
# Application level: setsockopt(TCP_QUICKACK) in Go


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION TRACKING (Tuning for high connection churn)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Conntrack table size
net.netfilter.nf_conntrack_max=1048576  # 1M connections

# Conntrack hash table size
# echo 262144 > /sys/module/nf_conntrack/parameters/hashsize

# Conntrack timeouts
net.netfilter.nf_conntrack_tcp_timeout_established=3600
net.netfilter.nf_conntrack_tcp_timeout_time_wait=30
```

#### D. RPS/RFS/XPS Setup Script

```bash
#!/bin/bash
# setup-packet-steering.sh

NIC="eth0"
NUM_CPUS=$(nproc)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS (Receive Side Scaling) - Hardware level
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Set RSS queues to match CPU count
ethtool -L $NIC combined $NUM_CPUS

# Verify
echo "RSS queues: $(ethtool -l $NIC | grep Combined | tail -1)"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RPS (Receive Packet Steering) - Software level
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# CPU mask for all CPUs (e.g., 16 CPUs = ffff)
CPU_MASK=$(printf '%x' $((2**NUM_CPUS - 1)))

echo "Setting RPS to all CPUs (mask: $CPU_MASK)"
for queue in /sys/class/net/$NIC/queues/rx-*; do
    echo "$CPU_MASK" > $queue/rps_cpus
    echo "  $(basename $queue): $CPU_MASK"
done


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# XPS (Transmit Packet Steering)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Bind each TX queue to specific CPU (1:1 mapping)
echo "Setting XPS (1 queue per CPU)"
TX_QUEUES=($(ls -d /sys/class/net/$NIC/queues/tx-* | wc -l))

for i in $(seq 0 $((NUM_CPUS-1))); do
    if [ -d "/sys/class/net/$NIC/queues/tx-$i" ]; then
        mask=$(printf '%x' $((1 << i)))
        echo "$mask" > /sys/class/net/$NIC/queues/tx-$i/xps_cpus
        echo "  tx-$i: CPU $i (mask: $mask)"
    fi
done


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RFS (Receive Flow Steering) - Keep flow on same CPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global RFS entries
echo 32768 > /proc/sys/net/core/rps_sock_flow_entries

# Per-queue RFS entries
echo "Setting RFS flow entries"
for queue in /sys/class/net/$NIC/queues/rx-*; do
    echo 2048 > $queue/rps_flow_cnt
done

echo "âœ… Packet steering configured!"
echo "Verify with: cat /sys/class/net/$NIC/queues/rx-0/rps_cpus"
```

#### E. NAPI (New API) Tuning

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NAPI CONFIGURATION (Interrupt â†’ Poll mode)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Packets processed per NAPI poll (before yielding)
net.core.netdev_budget=600              # Default: 300 (too low for throughput)

# Max time spent in NAPI poll (Î¼s)
net.core.netdev_budget_usecs=8000       # 8ms max poll time

# Busy polling (OPTIONAL - burns CPU for ultra-low latency)
# WARNING: Only use if you need <50Î¼s latency
# net.core.busy_poll=50                 # Poll 50Î¼s before sleep
# net.core.busy_read=50                 # Busy poll on socket read

# Per-socket busy poll (application can override)
# setsockopt(SO_BUSY_POLL, 50)
```

#### F. Application-Level Tuning (Go)

```go
// Enable SO_REUSEPORT (distribute accepts across threads)
listener, _ := net.Listen("tcp", ":8080")
file, _ := listener.(*net.TCPListener).File()
syscall.SetsockoptInt(int(file.Fd()), syscall.SOL_SOCKET, syscall.SO_REUSEPORT, 1)

// Enable TCP_NODELAY (disable Nagle - lower latency)
conn.(*net.TCPConn).SetNoDelay(true)

// Enable TCP_QUICKACK (immediate ACK, don't wait)
file, _ := conn.(*net.TCPConn).File()
syscall.SetsockoptInt(int(file.Fd()), syscall.IPPROTO_TCP, syscall.TCP_QUICKACK, 1)

// Set send/receive buffer sizes
conn.(*net.TCPConn).SetReadBuffer(4 * 1024 * 1024)   // 4MB
conn.(*net.TCPConn).SetWriteBuffer(4 * 1024 * 1024)  // 4MB

// TCP keepalive
conn.(*net.TCPConn).SetKeepAlive(true)
conn.(*net.TCPConn).SetKeepAlivePeriod(30 * time.Second)
```

#### G. Verification & Monitoring

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VERIFY SETTINGS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Check sysctl values
sysctl net.core.somaxconn
sysctl net.ipv4.tcp_tw_reuse
sysctl net.ipv4.tcp_congestion_control

# Check RPS/XPS settings
cat /sys/class/net/eth0/queues/rx-0/rps_cpus
cat /sys/class/net/eth0/queues/tx-0/xps_cpus

# Check RSS queues
ethtool -l eth0

# Check interrupts per CPU
cat /proc/interrupts | grep eth0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITOR PERFORMANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Socket statistics
ss -s

# Connection states
ss -tan | awk '{print $1}' | sort | uniq -c

# Dropped packets (should be 0!)
netstat -s | grep -i drop
ethtool -S eth0 | grep -i drop

# Retransmit rate (should be <1%)
netstat -s | grep -i retran

# TCP memory usage
cat /proc/net/sockstat

# IRQ balance (check interrupts spread across CPUs)
cat /proc/interrupts | grep eth0

# Network throughput
sar -n DEV 1 10  # Sample every 1s for 10s
```

#### H. Load Testing

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BENCHMARK BEFORE/AFTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Latency test (single connection)
wrk -t1 -c1 -d30s http://localhost:8080/health

# 2. Throughput test (many connections)
wrk -t16 -c1000 -d60s http://localhost:8080/health

# 3. Connection rate test
ab -n 100000 -c 100 http://localhost:8080/health

# 4. Monitor during test
watch -n1 'ss -s'
watch -n1 'netstat -s | grep -i retran'
```

**File Descriptor Limits:**

```bash
# /etc/security/limits.conf
* soft nofile 262144
* hard nofile 262144
* soft nproc 32768
* hard nproc 32768

# OR in systemd service
[Service]
LimitNOFILE=262144
LimitNPROC=32768
```

**Apply:**

```bash
# Apply sysctl
sudo sysctl -p /etc/sysctl.d/99-orchestrator.conf

# Run packet steering script
sudo ./setup-packet-steering.sh

# Verify
ulimit -n  # Should show 262144
```

**Expected Benefits:**

| Optimization | Latency Improvement | Throughput Improvement |
|--------------|---------------------|------------------------|
| tcp_tw_reuse | -20% (faster reuse) | +50% (more ports) |
| somaxconn=65536 | -10% (no drops) | +200% (accept burst) |
| BBR congestion control | -30% (better loss recovery) | +20% (better utilization) |
| TCP Fast Open (0-RTT) | -33% (1 RTT saved) | +15% (faster handshake) |
| RPS/XPS/RSS | -15% (better distribution) | +100% (multi-core) |
| Delayed ACK tuning | -40% (faster ACK) | +10% (less overhead) |
| NAPI budget=600 | -5% | +50% (more pkts/poll) |
| GRO/GSO | -10% CPU usage | +30% (batch processing) |

**Combined:** ~50-70% latency reduction, 2-3x throughput increase

**Monitoring:**

```bash
# Real-time connection stats
watch -n1 'ss -s'

# Check TIME_WAIT reuse
ss -tan state time-wait | wc -l  # Should drop quickly

# Packet drops (should be 0)
watch -n1 'ethtool -S eth0 | grep drop'

# IRQ distribution
watch -n1 'cat /proc/interrupts | grep eth0'

# TCP retransmits (should be <1%)
watch -n1 'netstat -s | grep retran'
```

---

### 3. GRO/GSO (Generic Receive/Segmentation Offload)

**Goal:** Reduce CPU usage for packet processing

**What:**
- **GRO**: Coalesce small packets into larger ones (receive)
- **GSO**: Split large segments into smaller packets (transmit)

**Configuration:**

```bash
# Check status (should be ON by default)
ethtool -k eth0 | grep -E 'generic-receive-offload|generic-segmentation-offload'

# Enable if not already (usually default)
ethtool -K eth0 gro on
ethtool -K eth0 gso on
```

**Benefits:**
- **20-30% CPU reduction** for network I/O
- Higher throughput (more data per syscall)
- Better for SSE/WS (long-lived streams)

**When to Disable:**
- Ultra-low latency requirements (<1ms)
- Busy-polling mode (not recommended for our use case)

---

### 4. SO_REUSEPORT

**Goal:** Distribute accepts across multiple threads

**Configuration:**

```go
// Set on listener sockets
listener, err := net.Listen("tcp", ":8080")
if err != nil {
    log.Fatal(err)
}

// Enable SO_REUSEPORT
file, _ := listener.(*net.TCPListener).File()
syscall.SetsockoptInt(int(file.Fd()), syscall.SOL_SOCKET, syscall.SO_REUSEPORT, 1)
```

**Benefits:**
- Multiple processes/threads bind to same port
- Kernel distributes accepts via load balancing
- **2-3x throughput** for high connection rate

---

### 5. HTTP/2 Multiplexing

**Goal:** Reduce connection overhead

**Configuration:**

```go
// Enable HTTP/2 (automatic in Go 1.6+)
server := &http.Server{
    Addr:    ":8080",
    Handler: router,
    // HTTP/2 enabled by default with TLS
}

// For HTTP/2 over cleartext (h2c)
h2s := &http2.Server{}
server.Handler = h2c.NewHandler(router, h2s)
```

**Benefits:**
- **Single connection per client** (vs. 6 in HTTP/1.1)
- Header compression
- Server push (for SSE/WS upgrade)

---

### 6. eBPF for Observability (Optional)

**Goal:** Deep system-level insights without code changes

**Use Cases:**

1. **HTTP latency histograms**
```bash
# bpftrace script (uprobe on Go net/http)
bpftrace -e '
uprobe:/opt/orchestrator/bin/orchestrator:"net/http.(*Transport).RoundTrip" {
    @start[tid] = nsecs;
}
uretprobe:/opt/orchestrator/bin/orchestrator:"net/http.(*Transport).RoundTrip" {
    $duration = (nsecs - @start[tid]) / 1000000;
    @latency_ms = hist($duration);
    delete(@start[tid]);
}'
```

2. **Per-run network attribution**
```bash
# cgroup-bpf: tag sockets with run_id
# Track bytes sent/received per workflow run
```

3. **Off-CPU profiling**
```bash
# Where time is spent (not on CPU)
sudo bpftrace offcpu.bt
```

**Benefits:**
- Zero application code changes
- <1% CPU overhead
- Production-safe

**Tools:**
- bpftrace
- bcc-tools
- Cilium (CNI)

**Documentation:** [../../performance-tuning.MD](../../performance-tuning.MD)

---

## Horizontal Scaling

### 1. Stateless Services (Easy Scaling)

**Scale independently:**

```bash
# Add more instances
kubectl scale deployment orchestrator --replicas=3
kubectl scale deployment http-worker --replicas=10
kubectl scale deployment agent-runner --replicas=5
```

**Load balancing:**
```
Clients â†’ HAProxy/Envoy â†’ Round-robin to instances
```

**Services that scale easily:**
- Orchestrator (API) - Stateless, scale to N
- HTTP Worker - Stateless, scale to N
- Agent Runner (Python) - Stateless, scale to N

---

### 2. Coordinator Scaling (Consumer Groups)

**Challenge:** Coordinator consumes from single queue

**Solution:** Redis Streams consumer groups

```go
// Create consumer group
redis.XGroupCreate("completion_signals", "coordinators", "0")

// Each coordinator joins the group
func (c *Coordinator) Start() {
    for {
        // XREADGROUP: Only one consumer gets each message
        msgs := redis.XReadGroup("coordinators", "coord-1",
            "completion_signals", ">", 1)

        for _, msg := range msgs {
            c.handleCompletion(msg)
            redis.XAck("completion_signals", "coordinators", msg.ID)
        }
    }
}
```

**Scaling:**
```
1 coordinator: 1,000 workflows/sec
3 coordinators: 3,000 workflows/sec (linear)
10 coordinators: 10,000 workflows/sec
```

**Monitoring:**
```bash
# Check consumer group status
redis-cli XINFO GROUPS completion_signals

# Check pending messages
redis-cli XPENDING completion_signals coordinators
```

---

### 3. Fanout Scaling (Sticky Sessions)

**Challenge:** WebSocket connections are stateful

**Solution:** Sticky sessions by run_id

```nginx
# HAProxy config
frontend websocket
    bind *:8090
    use_backend fanout_servers

backend fanout_servers
    balance url_param run_id  # Sticky by run_id
    hash-type consistent
    server fanout1 fanout-1:8090 check
    server fanout2 fanout-2:8090 check
    server fanout3 fanout-3:8090 check
```

**OR: Redis Pub/Sub (all instances listen)**

```go
// All fanout instances subscribe to same channel
pubsub := redis.Subscribe("run:" + runID)

// Publish once, all instances receive
redis.Publish("run:" + runID, event)
```

**Scaling:**
```
1 fanout: 10,000 connections
3 fanout: 30,000 connections
10 fanout: 100,000 connections
```

---

### 4. Database Scaling

**Read Replicas:**

```sql
-- Write to primary
INSERT INTO runs (...) VALUES (...);

-- Read from replicas
SELECT * FROM runs_read WHERE run_id = '...';
```

**Connection Pooling:**

```go
// pgx pool config
config, _ := pgxpool.ParseConfig(os.Getenv("DATABASE_URL"))
config.MaxConns = 100
config.MinConns = 10
config.MaxConnLifetime = time.Hour
config.MaxConnIdleTime = 30 * time.Minute

pool, _ := pgxpool.ConnectConfig(context.Background(), config)
```

**Partitioning (future):**

```sql
-- Partition by run_id
CREATE TABLE runs (
    run_id UUID PRIMARY KEY,
    ...
) PARTITION BY HASH (run_id);

CREATE TABLE runs_0 PARTITION OF runs FOR VALUES WITH (MODULUS 4, REMAINDER 0);
CREATE TABLE runs_1 PARTITION OF runs FOR VALUES WITH (MODULUS 4, REMAINDER 1);
CREATE TABLE runs_2 PARTITION OF runs FOR VALUES WITH (MODULUS 4, REMAINDER 2);
CREATE TABLE runs_3 PARTITION OF runs FOR VALUES WITH (MODULUS 4, REMAINDER 3);
```

---

### 5. Redis Scaling

**Single instance (MVP):**
```
100K ops/sec
```

**Redis Cluster (production):**
```
# 3 masters + 3 replicas
Master 1 (slots 0-5461)     â†’ Replica 1
Master 2 (slots 5462-10922) â†’ Replica 2
Master 3 (slots 10923-16383) â†’ Replica 3

Throughput: 300K ops/sec
```

**Configuration:**

```bash
# redis.conf
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
appendfsync everysec
```

**Client:**

```go
// redis-go-cluster
client := redis.NewClusterClient(&redis.ClusterOptions{
    Addrs: []string{
        "redis-1:6379",
        "redis-2:6379",
        "redis-3:6379",
    },
    PoolSize: 100,
})
```

---

## Performance Tuning

### 1. Kafka Tuning (Future)

**Producer:**

```properties
# Idempotent writes
enable.idempotence=true
acks=all

# Batching
linger.ms=5
batch.size=16384

# Compression
compression.type=snappy

# Partitions (per topic)
num.partitions=64

# Consistent hashing partitioner
partitioner.class=org.apache.kafka.clients.producer.ConsistentAssignmentPartitioner
```

**Consumer:**

```properties
# Batch processing
max.poll.records=500
fetch.min.bytes=1024
fetch.max.wait.ms=500

# Offset management
enable.auto.commit=false
isolation.level=read_committed
```

**Benefits:**
- **10x throughput** vs. Redis Streams
- Durable, replicated
- Exactly-once semantics

#### Pre-emptive Materialization via Kafka

**Current Bottleneck:**
Synchronous materialization blocks API requests:
```
POST /patch â†’ Validate â†’ Materialize (50ms) â†’ Store â†’ Return
                          â†‘ Blocks here
```

**Solution: Event-Driven Materialization**

```
POST /patch â†’ Validate â†’ Publish to Kafka â†’ Return 202 Accepted
                              â†“
                    Kafka: patch.created (partitioned by workflow_id)
                              â†“
              Materialization Workers (consume in batches)
                              â†“
              Aggregate 100 patches OR 5s window
                              â†“
              Batch materialize â†’ Bulk INSERT
```

**Kafka Topic Configuration:**

```properties
# Topic: patch.created
num.partitions=32
replication.factor=3
compression.type=snappy
retention.ms=604800000  # 7 days
segment.ms=86400000     # 1 day segments

# Consistent hashing ensures workflow patches go to same partition
# Prevents split-brain materialization for same workflow
```

**Consumer Group (Materialization Workers):**

```go
package main

import (
    "context"
    "github.com/segmentio/kafka-go"
)

func startMaterializationWorker(workerID int) {
    reader := kafka.NewReader(kafka.ReaderConfig{
        Brokers: []string{"kafka-1:9092", "kafka-2:9092", "kafka-3:9092"},
        Topic:   "patch.created",
        GroupID: "materialization-workers",

        // Batch configuration
        MinBytes: 1024,              // Wait for at least 1KB
        MaxBytes: 10485760,          // Max 10MB per batch
        MaxWait:  5 * time.Second,   // Or wait 5s max
    })

    for {
        // Fetch batch of messages
        ctx := context.Background()
        messages := reader.FetchMessage(ctx)

        // Group by workflow_id for efficient materialization
        grouped := groupByWorkflow(messages)

        // Process each workflow's patches in batch
        for workflowID, patches := range grouped {
            // Apply all patches at once
            materializedWorkflow := applyPatchesBatch(baseWorkflow, patches)

            // Single database transaction for all patches
            err := bulkInsertMaterialized(workflowID, materializedWorkflow)
            if err != nil {
                log.Error("materialization failed", "workflow", workflowID, "error", err)
                continue
            }

            log.Info("batch materialized",
                "workflow", workflowID,
                "patches", len(patches),
                "duration_ms", time.Since(start).Milliseconds(),
            )
        }

        // Commit offsets after successful processing
        reader.CommitMessages(ctx, messages...)
    }
}
```

**Consistent Hashing Benefits:**

```
Without Consistent Hashing:
  Workflow A patches: P0, P2, P5, P7  â† Spread across partitions
  Problem: Different workers process, race conditions!

With Consistent Hashing:
  Workflow A: hash("workflow-A") % 32 â†’ Always partition 7
  All patches for Workflow A â†’ Partition 7 â†’ Same worker
  Result: Sequential processing, no races! âœ…
```

**Performance Improvement:**

```
Synchronous (Current):
  1 patch = 50ms (materialize + store)
  100 patches = 5000ms (5 seconds!)

Event-Driven Batch:
  100 patches â†’ Kafka (5ms)
  Consumer aggregates â†’ Materialize once (60ms)
  Bulk INSERT (40ms)
  Total: 105ms for 100 patches (47x faster!)
```

**Monitoring:**

```bash
# Consumer lag (patches waiting)
kafka-consumer-groups --describe --group materialization-workers

# Should be near 0 under normal load
# If lag > 1000, scale up workers

# Partition distribution
kafka-topics --describe --topic patch.created
# Verify even distribution across brokers
```

---

### 2. Postgres Tuning

**Configuration:**

```sql
-- postgresql.conf

# Memory
shared_buffers = 8GB
effective_cache_size = 24GB
work_mem = 64MB
maintenance_work_mem = 2GB

# Checkpoints
checkpoint_completion_target = 0.9
wal_buffers = 16MB
max_wal_size = 4GB
min_wal_size = 1GB

# WAL compression
wal_compression = on

# Autovacuum
autovacuum_max_workers = 4
autovacuum_naptime = 10s

# Connections
max_connections = 200
```

**Outbox Pattern (for Kafka):**

```sql
-- Batch fetch with SKIP LOCKED
SELECT * FROM outbox
WHERE status = 'pending'
ORDER BY created_at
LIMIT 1000
FOR UPDATE SKIP LOCKED;
```

**Benefits:**
- **Higher throughput** (batch operations)
- Lower contention (SKIP LOCKED)
- WAL compression (less I/O)

---

### 3. CAS Optimization

**Content-Addressed Storage:**

```
All blobs keyed by sha256
â†’ Automatic deduplication
â†’ Immutable (never overwrite)
â†’ Parallel access (no locks)
```

**S3/MinIO Config:**

```go
// Multipart uploads (large files)
uploader := s3manager.NewUploader(sess, func(u *s3manager.Uploader) {
    u.PartSize = 10 * 1024 * 1024  // 10MB parts
    u.Concurrency = 10
})

// Server-side compression
_, err := s3.PutObject(&s3.PutObjectInput{
    Bucket:               aws.String("workflow-cas"),
    Key:                  aws.String(casID),
    Body:                 bytes.NewReader(data),
    ServerSideEncryption: aws.String("AES256"),
    ContentEncoding:      aws.String("gzip"),
})
```

---

### 4. Proximity-Based Intelligent Caching

**Concept:** Cache node execution results across users in the same workspace/team

**Problem:**
```
Team Data Engineering (5 users):
  - User A: Runs "fetch_api_data" node at 9:00 AM
  - User B: Runs same node with same inputs at 9:05 AM
  - User C: Runs same node at 9:10 AM

Without caching: 3 identical API calls (wasteful!)
With proximity caching: 1 API call + 2 cache hits (efficient!) âœ¨
```

**Cache Key Algorithm:**

```go
type CacheKey struct {
    WorkspaceID      string  // "acme-corp-data-team"
    PermissionLevel  string  // "admin", "write", "read"
    NodeType         string  // "http_request", "agent", "function"
    NodeConfigHash   string  // Hash of node configuration
    InputsHash       string  // Hash of input parameters
}

func computeCacheKey(token Token) string {
    return sha256(fmt.Sprintf(
        "%s:%s:%s:%s:%s",
        token.WorkspaceID,
        token.PermissionLevel,
        token.NodeType,
        hashConfig(token.NodeConfig),
        hashInputs(token.Inputs),
    ))
}
```

**Permission-Aware Cache Lookup:**

```go
func (w *Worker) ExecuteWithCache(token Token) (Result, error) {
    // 1. Compute cache key
    cacheKey := computeCacheKey(token)

    // 2. Check cache (includes permission verification)
    cached, found := w.cache.Get(cacheKey, token.UserID, token.WorkspaceID)
    if found {
        // Verify user has permission to access this cached result
        if !hasPermission(token.UserID, cached.WorkspaceID, cached.RequiredPermission) {
            // Permission denied - cache miss
            goto execute
        }

        // Cache hit! Increment hit counter
        w.cache.IncrementHits(cacheKey)
        w.metrics.RecordCacheHit(token.NodeType, token.WorkspaceID)

        log.Info("proximity cache hit",
            "workspace", token.WorkspaceID,
            "node_type", token.NodeType,
            "saved_ms", cached.OriginalExecutionTime,
        )

        return cached.Result, nil
    }

execute:
    // 3. Cache miss - execute node
    startTime := time.Now()
    result := w.executeNode(token)
    executionTime := time.Since(startTime)

    // 4. Store in cache with workspace isolation
    w.cache.Set(cacheKey, CacheEntry{
        WorkspaceID:          token.WorkspaceID,
        RequiredPermission:   token.PermissionLevel,
        Result:               result,
        OriginalExecutionTime: executionTime.Milliseconds(),
        TTL:                  getTTL(token.NodeType),
    })

    return result, nil
}
```

**Semantic Caching for Agent Nodes:**

For LLM-based agent nodes, use embedding similarity instead of exact match:

```python
class AgentSemanticCache:
    """
    Semantic cache for agent node prompts.
    Matches similar prompts across workspace users.
    """

    def __init__(self, db, workspace_id):
        self.db = db
        self.workspace = workspace_id
        self.embedding_model = "text-embedding-3-small"  # 1536 dimensions
        self.similarity_threshold = 0.92  # High confidence required

    def check_cache(self, prompt: str, node_config: dict, user_id: str) -> Optional[dict]:
        # 1. Compute embedding for the prompt
        embedding = get_embedding(prompt, self.embedding_model)

        # 2. Vector similarity search in workspace cache
        query = """
            SELECT cache_id, result_cas_ref,
                   1 - (prompt_embedding <=> $1::vector) AS similarity
            FROM agent_semantic_cache
            WHERE workspace_id = $2
              AND node_type = $3
              AND expires_at > now()
              AND 1 - (prompt_embedding <=> $1::vector) > $4
            ORDER BY similarity DESC
            LIMIT 1;
        """

        result = self.db.execute(query,
            embedding,
            self.workspace,
            node_config['type'],
            self.similarity_threshold
        )

        if result:
            # Verify user has permission
            if not has_permission(user_id, result.workspace_id, result.required_permission):
                return None

            # Cache hit!
            log.info(f"Semantic cache hit (similarity: {result.similarity:.3f})")
            self.db.execute("UPDATE agent_semantic_cache SET hit_count = hit_count + 1 WHERE cache_id = $1", result.cache_id)
            return load_from_cas(result.result_cas_ref)

        return None  # Cache miss

    def store(self, prompt: str, result: dict, node_config: dict, execution_time_ms: int):
        embedding = get_embedding(prompt, self.embedding_model)

        self.db.execute("""
            INSERT INTO agent_semantic_cache (
                workspace_id, node_type, prompt_text, prompt_embedding,
                result_cas_ref, required_permission, original_execution_time_ms, expires_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, now() + interval '24 hours')
        """,
            self.workspace,
            node_config['type'],
            prompt,
            embedding,
            store_in_cas(result),
            get_user_permission_level(),
            execution_time_ms
        )
```

**Storage: pgvector Extension**

```sql
-- Enable vector extension
CREATE EXTENSION vector;

-- Cache table with vector embeddings
CREATE TABLE agent_semantic_cache (
    cache_id UUID PRIMARY KEY DEFAULT uuid_generate_v7(),
    workspace_id UUID NOT NULL,
    node_type TEXT NOT NULL,
    prompt_text TEXT NOT NULL,
    prompt_embedding vector(1536),  -- OpenAI text-embedding-3-small
    result_cas_ref TEXT NOT NULL,
    required_permission TEXT NOT NULL,  -- 'read', 'write', 'admin'
    hit_count INT DEFAULT 0,
    original_execution_time_ms INT,
    created_at TIMESTAMPTZ DEFAULT now(),
    expires_at TIMESTAMPTZ NOT NULL,
    created_by_user_id UUID
);

-- Vector similarity index (IVFFlat for fast ANN search)
CREATE INDEX agent_cache_embedding_idx ON agent_semantic_cache
USING ivfflat (prompt_embedding vector_cosine_ops)
WITH (lists = 100);

-- Workspace + expiry index
CREATE INDEX agent_cache_workspace_idx ON agent_semantic_cache
(workspace_id, expires_at) WHERE expires_at > now();

-- Permission check function
CREATE FUNCTION can_access_cache(
    p_user_id UUID,
    p_workspace_id UUID,
    p_required_permission TEXT
) RETURNS BOOLEAN AS $$
    SELECT EXISTS (
        SELECT 1 FROM users
        WHERE user_id = p_user_id
          AND workspace_id = p_workspace_id
          AND permission_level >= p_required_permission  -- 'admin' > 'write' > 'read'
    );
$$ LANGUAGE SQL STABLE;
```

**Performance Comparison:**

```
Scenario: 10 users in "acme-data" workspace run similar agent prompts

Without Semantic Cache:
  10 users Ã— 500ms LLM call = 5000ms total
  Cost: 10 Ã— $0.0001 = $0.001 per batch

With Semantic Cache (90% hit rate):
  1st user: 500ms (cache miss, store)
  9 users: 9 Ã— 1ms (cache hits) = 9ms
  Total: 509ms (10x faster!)
  Cost: 1 Ã— $0.0001 = $0.0001 (10x cheaper!)
```

**Cache Hit Metrics:**

```go
// Track cache effectiveness
type CacheMetrics struct {
    Hits         int64  // Number of cache hits
    Misses       int64  // Number of cache misses
    Savings      int64  // Total milliseconds saved
    CostSavings  float64 // Estimated $ saved (for LLM calls)
}

// Report per workspace
func (m *MetricsCollector) ReportCacheStats(workspaceID string) {
    stats := m.getCacheStats(workspaceID)

    log.Info("cache stats",
        "workspace", workspaceID,
        "hit_rate", float64(stats.Hits) / float64(stats.Hits + stats.Misses),
        "time_saved_sec", stats.Savings / 1000,
        "cost_saved_usd", stats.CostSavings,
    )
}
```

**TTL Strategy (by node type):**

```
Deterministic nodes (always same output):
  - function: 7 days
  - pure computation: 30 days

External dependencies (may change):
  - http_request: 1 hour
  - database_query: 5 minutes

LLM-based (semantic stability):
  - agent (semantic cache): 24 hours
  - agent (exact match): 7 days
```

**Security & Isolation:**

1. **Workspace boundaries** - User A in workspace X cannot see cache from workspace Y
2. **Permission levels** - "read" user cannot access cache from "admin" operation
3. **Audit trail** - Log who used whose cache (for compliance)
4. **PII detection** - Don't cache results containing sensitive patterns

**Benefits:**
- âœ… **10x speedup** for repeated operations in same team
- âœ… **Cost savings** - Avoid redundant LLM/API calls
- âœ… **Workspace collaboration** - Teams benefit from each other's work
- âœ… **Intelligent** - Semantic matching for agents, not just exact match
- âœ… **Secure** - Permission-aware, workspace-isolated

---

### 5. Client-Side Materialization (Web Workers)

**Purpose:** Offload CPU-intensive materialization from server to browser

**The Scalability Problem:**

When users view workflows with many patches, server CPU is wasted on presentation logic:

```
Current (Server-Side):
  10 users viewing workflows with 100 patches each
  â†’ 10 Ã— 500ms server CPU = 5 seconds of CPU time
  â†’ Blocks server threads
  â†’ Reduces capacity for actual workflow execution

Problem gets worse with scale:
  1000 concurrent users viewing workflows
  â†’ Server CPUs maxed out just rendering UIs!
  â†’ Need more servers just for materialization
```

**Solution: Client-Side with Web Workers**

```javascript
// Browser does the work (separate thread, doesn't block UI)
const worker = new Worker('materializer.worker.js');
worker.postMessage({base, patches});

// Worker materializes in background
worker.onmessage = (result) => {
    renderWorkflow(result.workflow);  // UI updates when ready
};
```

**Scalability Benefits:**

```
Client-Side Approach:
  1000 users viewing workflows
  â†’ 1000 browsers Ã— 500ms = Distributed across 1000 CPUs!
  â†’ Server CPU usage: ~0ms for materialization
  â†’ Server handles only API requests (fast!)
  â†’ Horizontal scaling for free (users bring their own CPU)
```

**Performance at Scale:**

| Users | Server CPU (Old) | Server CPU (New) | Scaling Cost |
|-------|------------------|------------------|--------------|
| 10 | 5s | 0.1s (API only) | **50x reduction** |
| 100 | 50s | 1s | **50x reduction** |
| 1000 | 500s (8 cores!) | 10s | **50x reduction** |

**Additional Benefits:**

1. **No server capacity waste** - Server focuses on workflow execution, not UI rendering
2. **Natural load distribution** - Each browser uses its own CPU
3. **Faster for users** - No network roundtrip for materialized result
4. **Mobile-friendly** - Even phones can materialize workflows
5. **Offline capable** - Can cache base + patches, materialize offline

**When This is Critical:**

- **High user count**: 1000+ concurrent UI users
- **Complex workflows**: 50+ patches per workflow
- **Edge cases**: Compaction failures leaving 100+ patches
- **Cost optimization**: Reduce server instance count

**Future: WASM for 10-50x Faster Materialization**

See [VISION.md](#wasm-optimizer-for-client-side-materialization) for Rust/WASM implementation that makes this even faster.

---

### 6. Dragonfly Cache (Future)

**Configuration:**

```bash
# Launch Dragonfly (Redis-compatible, 25x faster)
dragonfly --port 6379 --maxmemory 16gb --eviction allkeys-lru
```

**Benefits:**
- **25x throughput** vs. Redis (vertical scaling)
- Multi-threaded (uses all cores)
- Lower latency (microseconds)

**Use Case:**
- Memoized node results
- Session state
- Metadata cache

---

## Monitoring & Observability

### 1. Metrics (Prometheus)

```go
var (
    workflowsStarted = prometheus.NewCounterVec(
        prometheus.CounterOpts{Name: "workflows_started_total"},
        []string{"tag"},
    )

    nodeExecutions = prometheus.NewCounterVec(
        prometheus.CounterOpts{Name: "node_executions_total"},
        []string{"type", "status"},
    )

    hopLatency = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "hop_latency_ms",
            Buckets: []float64{1, 5, 10, 50, 100, 500, 1000},
        },
        []string{"from_type", "to_type"},
    )

    redisOps = prometheus.NewCounterVec(
        prometheus.CounterOpts{Name: "redis_ops_total"},
        []string{"operation", "status"},
    )
)
```

**Grafana Dashboards:**
```
- Workflow throughput (per second)
- Node execution latency (P50/P95/P99)
- Redis operations (ops/sec)
- Postgres query latency
- Agent LLM calls (cost tracking)
- Error rates (by service)
```

---

### 2. Alerts (Prometheus Alertmanager)

```yaml
groups:
  - name: orchestrator_alerts
    rules:
      - alert: WorkflowStalled
        expr: workflow_counter > 0 AND rate(node_executions_total[5m]) == 0
        for: 10m
        annotations:
          summary: "Workflow {{ $labels.run_id }} appears stalled"

      - alert: HighLatency
        expr: histogram_quantile(0.95, hop_latency_ms) > 1000
        for: 5m
        annotations:
          summary: "P95 hop latency > 1000ms"

      - alert: PostgresConnectionPoolExhausted
        expr: pg_connections_active / pg_connections_max > 0.9
        for: 2m
        annotations:
          summary: "Postgres connection pool at 90%"

      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        annotations:
          summary: "Redis memory usage at 90%"
```

---

### 3. Distributed Tracing (OpenTelemetry)

```go
import "go.opentelemetry.io/otel"

func (w *Worker) Execute(token Token) error {
    ctx, span := otel.Tracer("workflow").Start(
        context.Background(),
        "node.execute",
        trace.WithAttributes(
            attribute.String("run_id", token.RunID),
            attribute.String("node_id", token.ToNode),
            attribute.Int("hop", token.Hop),
        ),
    )
    defer span.End()

    // Propagate trace_id in token
    token.TraceID = span.SpanContext().TraceID().String()

    // Execute node
    result, err := w.executeNode(ctx, token)

    if err != nil {
        span.RecordError(err)
    }

    return err
}
```

**Jaeger/Tempo:**
- Visualize per-run execution paths
- Identify slow nodes
- Track cross-service calls

---

## Production Deployment

### 1. Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer (HAProxy)          â”‚
â”‚  â”œâ”€ TLS termination                      â”‚
â”‚  â”œâ”€ Rate limiting (global)               â”‚
â”‚  â””â”€ Health checks                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ API-1  â”‚ â”‚ API-2  â”‚ â”‚ API-3  â”‚  (Orchestrator)
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
       â”‚   Postgres  â”‚  (Primary + Replicas)
       â”‚   Redis     â”‚  (Cluster)
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚Coord-1 â”‚ â”‚Coord-2 â”‚ â”‚Coord-3 â”‚  (Coordinator)
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚       Worker Tier          â”‚
â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”     â”‚
â”‚  â”‚HTTPâ”‚ â”‚HITLâ”‚ â”‚Agntâ”‚ ... â”‚
â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. Deployment Checklist

**Infrastructure:**
- [ ] PostgreSQL (primary + read replicas)
- [ ] Redis (cluster mode, 3 masters + 3 replicas)
- [ ] HAProxy/Envoy (load balancer)
- [ ] Prometheus + Grafana (monitoring)
- [ ] Jaeger/Tempo (tracing)

**Services:**
- [ ] Orchestrator (3+ instances)
- [ ] Coordinator (3+ instances, consumer group)
- [ ] HTTP Worker (10+ instances)
- [ ] Agent Runner (5+ instances)
- [ ] HITL Worker (2+ instances)
- [ ] Fanout (3+ instances, sticky sessions)

**OS Tuning:**
- [ ] Sysctl tuning applied
- [ ] Systemd slices configured
- [ ] CPU pinning enabled
- [ ] File descriptor limits set
- [ ] Network stack tuned

**Monitoring:**
- [ ] Metrics exporters running
- [ ] Dashboards created
- [ ] Alerts configured
- [ ] Log aggregation (ELK/Loki)
- [ ] Distributed tracing

---

### 3. Capacity Planning

**Per-instance capacity:**

| Service | Instances | Throughput | Notes |
|---------|-----------|-----------|-------|
| Orchestrator | 3 | 15K req/sec | CPU-bound |
| Coordinator | 3 | 3K workflows/sec | Redis-bound |
| HTTP Worker | 10 | 100K req/sec | Network-bound |
| Agent Runner | 5 | 500 decisions/sec | OpenAI API-bound |
| Fanout | 3 | 30K connections | Memory-bound |
| Postgres | 1 primary + 2 replicas | 15K writes/sec | Disk-bound |
| Redis | 3 masters | 300K ops/sec | Memory-bound |

**Cost estimate (AWS):**
```
Compute (EC2): $2000/month (10 instances, m5.2xlarge)
Database (RDS): $1000/month (db.r5.2xlarge)
Redis (ElastiCache): $500/month (cache.r5.xlarge Ã— 3)
S3 (CAS): $100/month (100GB)
OpenAI (Agent LLM): Variable ($1000-5000/month)

Total: ~$5000-9000/month for 10K workflows/sec
```

---

## Summary

**Current (MVP):**
- 1,000 workflows/sec
- Single-instance components
- Redis Streams + Postgres
- Basic monitoring

**Production Target:**
- 10,000 workflows/sec
- Horizontally scaled (3-10 instances)
- Kafka + Redis Cluster + Postgres sharding
- Full observability (metrics, logs, traces)
- OS-level tuning (CPU pinning, network stack)

**Migration Path:**
- Incremental (no downtime)
- Add capacity before removing old
- Measure at each step
- Roll back if needed

**Documentation:**
- [../../performance-tuning.MD](../../performance-tuning.MD) - Performance guide
- [../../scripts/systemd/README.md](../../scripts/systemd/README.md) - Systemd configs
- [../architecture/VISION.md](../architecture/VISION.md) - Production architecture

---

**All architectural decisions support 100x scaling from MVP to production!**
