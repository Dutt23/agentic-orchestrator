ğŸ§  Agentic Orchestration Builder â€” System Overview

1. Vision & Philosophy

Build an agentic, event-driven orchestration engine that merges deterministic workflows (like n8n/Temporal) with adaptive, reasoning-driven systems (like multi-agent AI setups).

Instead of static DAGs, the system allows agents and optimizers to propose patches to the running graph â€” safely, observably, and without breaking determinism.

At its core:

â€¢ Deterministic event sourcing for reproducibility
â€¢ Agentic overlay patches for adaptability
â€¢ Event-driven execution via Kafka/Redis
â€¢ SSE/WebSockets for real-time observability
â€¢ WASM optimization for runtime graph fusion
â€¢ OS-level tuning for high-throughput execution


2. Core Architectural Layers

Control Plane
Service              Role
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Edge Proxy           TLS termination, rate-limiting, SSE/WS upgrade
API Service          Entry for CLI/UI requests (Start/Cancel/Replay)
Orchestrator         Workflow resolution: base + patches â†’ materialized
Validator            Validates agent patches, enforces limits
Optimizer (WASM)     Rewrites subgraphs (fusion, pruning, caching)


Data Plane
Service              Role
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Runner               Executes deterministic nodes (function/map/join)
Agent Runner         K8s/Lambda/customer env for LLM execution
Fanout               Real-time SSE/WS streaming to UI/CLI
HITL Service         Human approval gates with pause/resume


Infrastructure
Component            Role
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Kafka / Redpanda     Main event bus and work queues
Postgres             Run metadata, patches, event log
S3 / MinIO (CAS)     Content-addressed artifact storage
Dragonfly / Redis    Cache for memoized node results


3. Data Flow (Start to Finish)

User Submits Run
â”œâ”€ CLI or UI sends workflow submission
â”œâ”€ API validates inputs
â””â”€ API resolves workflow tag â†’ artifact_id

Orchestrator Resolution
â”œâ”€ Load base workflow from database
â”œâ”€ Fetch existing patches for this workflow
â”œâ”€ Materialize: base + patchâ‚ + patchâ‚‚ â†’ executable workflow
â”œâ”€ Compile to IR (Intermediate Representation)
â””â”€ Store IR in cache (Redis: ir:{run_id})

Initialize Execution
â”œâ”€ Create run record in Postgres
â”œâ”€ Publish first tokens to entry nodes
â””â”€ Coordinator begins listening for completions

Execution Loop (Choreography)
â”œâ”€ Worker consumes token from queue
â”œâ”€ Worker loads config from CAS
â”œâ”€ Worker executes business logic
â”œâ”€ Worker stores output in CAS
â”œâ”€ Worker publishes completion signal
â”œâ”€ Coordinator loads IR (may be patched!)
â”œâ”€ Coordinator determines next nodes
â””â”€ Coordinator publishes tokens to next workers

Agent Patch Flow
â”œâ”€ Agent worker calls LLM with context
â”œâ”€ LLM decides to modify workflow
â”œâ”€ LLM generates JSON Patch operations
â”œâ”€ Agent submits: POST /runs/{run_id}/patch
â”œâ”€ Validator checks syntax + agent limits
â”œâ”€ Orchestrator applies patch â†’ new artifact
â”œâ”€ Orchestrator recompiles IR
â”œâ”€ Orchestrator updates cache: ir:{run_id}
â”œâ”€ Agent publishes completion signal
â”œâ”€ Coordinator loads NEW IR
â””â”€ Coordinator routes to NEW nodes!

HITL Approval Flow
â”œâ”€ HITL node triggered
â”œâ”€ Worker creates approval record
â”œâ”€ Workflow pauses (counter=0, pending approval)
â”œâ”€ Human reviews in UI or CLI: aob approve ticket_id
â”œâ”€ Decision submitted â†’ workflow resumes
â””â”€ Tokens published to approve/reject paths

Optimizer Flow
â”œâ”€ Optimizer (WASM) analyzes IR
â”œâ”€ Finds optimization opportunities (HTTP fusion, etc.)
â”œâ”€ Generates OptimizedPatch
â”œâ”€ Submits as patch (same validation as agent patches)
â”œâ”€ Applied to workflow
â””â”€ Future runs use optimized topology

Completion Detection
â”œâ”€ Coordinator checks counter == 0
â”œâ”€ Coordinator checks no pending HITL
â”œâ”€ Mark run as COMPLETED
â””â”€ Cleanup ephemeral state


4. Communication Model

Work Dispatch: Kafka topics (partitioned by run_id)
â”œâ”€ node.jobs.high (high-priority work)
â”œâ”€ node.jobs.medium (medium-priority work)
â”œâ”€ node.jobs.low (low-priority work)
â””â”€ node.results (execution outputs)

Real-time: SSE/WebSocket (Fanout service)
â”œâ”€ Timeline updates
â”œâ”€ Log streaming
â”œâ”€ Approval notifications
â””â”€ Patch diffs

Service-to-Service: HTTP REST (MVP) â†’ gRPC (production)


5. Caching & Memoization

Each node has a caching policy:
â”œâ”€ off: No caching
â”œâ”€ read_only: Check cache, never write
â”œâ”€ read_through: Check cache, write on miss
â””â”€ write_only: Always write to cache

Cache stored in Dragonfly with metadata + CAS pointers.

Safe to reuse only for deterministic, side-effect-free nodes.

WASM optimizer identifies cacheable subgraphs automatically.


6. Error Handling & Retry

Runner Failures
â”œâ”€ Transient error â†’ exponential backoff retry
â”œâ”€ Deterministic failure â†’ emit NodeFailed event
â””â”€ Orchestrator decides: retry / skip / HITL review

Agent Failures
â”œâ”€ Patch validation error â†’ return to LLM with examples
â”œâ”€ LLM retry (self-correction)
â””â”€ If retry fails â†’ store invalid patch for manual review

Agent Spawn Protection
â”œâ”€ Layer 1 (Python): Check before LLM call
â”œâ”€ Layer 2 (Go): Validate patch operations
â”œâ”€ Layer 3 (Coordinator): Security check during routing
â””â”€ Max 5 agents per workflow (configurable)

Queue Backpressure
â”œâ”€ Kafka partitions scale horizontally
â”œâ”€ Workers bounded concurrency
â””â”€ Rate limiting (workflow-aware tiers)

Timeouts
â”œâ”€ Durable timers in Postgres
â”œâ”€ RunTimeout events trigger cleanup
â””â”€ HITL timeouts â†’ auto-approve or escalate


7. OS & Network Optimizations

CPU Pinning
â”œâ”€ Control Plane (API, Orchestrator, Validator): Cores 0-7
â”œâ”€ Runner Plane (Runners, Agents): Cores 8-15
â””â”€ Fanout Plane (Streaming): Cores 16-19

GOMAXPROCS set per service for controlled concurrency.

Network Stack
â”œâ”€ SO_REUSEPORT: Distribute accepts across threads
â”œâ”€ HTTP/2 multiplexing for SSE/WS
â”œâ”€ GRO/GSO enabled (packet batching, 20-30% CPU reduction)
â””â”€ sysctl tuning: backlog, port ranges, TCP timeouts

Kafka Tuning
â”œâ”€ 64â€“256 partitions per topic
â”œâ”€ Idempotent producers (exactly-once)
â”œâ”€ linger.ms batching (5ms)
â””â”€ Compression (snappy)

Postgres Tuning
â”œâ”€ WAL compression
â”œâ”€ Skip-locked outbox reads (FOR UPDATE SKIP LOCKED)
â””â”€ Connection pooling (100 max conns)

Deployment Modes
â”œâ”€ Fast Path: minimal tracing, max throughput
â””â”€ Full Fidelity: full audit, overlays, HITL, replays


8. Agent Orchestration (Core Innovation)

How Agents Resolve Workflow Changes:

Step 1: Materialization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Base Workflow (v1.0)
  + Agent Patch 1 (adds email node)
  + Agent Patch 2 (adds validation)
  + Optimizer Patch (fuses HTTP nodes)
  = Materialized Workflow (v1.3)

Step 2: Compilation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Materialized Workflow â†’ IR Compiler â†’ Intermediate Representation
â†’ Stores in cache: Redis SET ir:{run_id} {json_ir}

Step 3: Execution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Coordinator loads IR on every completion signal
â†’ IR may have changed (patched by agent!)
â†’ Determines next nodes from NEW topology
â†’ Routes to workers
â†’ Workflow continues with modified graph!

This is the key: Coordinator ALWAYS loads latest IR, so patches apply immediately.


9. Developer CLI (aob)

Fast, lightweight Rust CLI for workflow management:

aob run start workflow.json
  â†’ Start workflow execution

aob logs stream <run_id>
  â†’ Real-time SSE log streaming

aob logs stream <run_id> --node enrich
  â†’ Filter by specific node

aob approve <ticket> approve --reason "LGTM"
  â†’ HITL approval from terminal

aob patch list <run_id>
  â†’ List agent-proposed patches

aob patch show <patch_id>
  â†’ Show patch diff

aob replay <run_id> --from parse
  â†’ Replay from checkpoint

Features:
â”œâ”€ 2.4MB binary (Rust, optimized)
â”œâ”€ <10ms startup time
â”œâ”€ Real-time SSE streaming (not polling)
â”œâ”€ JSON output for scripting
â””â”€ Complete workflow lifecycle management


10. Innovation Summary

Area                          Innovation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Graph Patching                Agentic overlays (not mutations!)
Agent Resolution              Materialization â†’ Recompilation â†’ Execution
Hybrid Model                  Deterministic backbone + agent adaptability
Agent Safety                  Triple-layer validation (Python, Go, Coordinator)
WASM Optimizer                Dynamic graph fusion without redeploy
Rate Limiting                 Workflow-aware tiers (complexity-based)
Coordinator                   Stateless (crash-resume without data loss)
Execution Environments        K8s, Lambda, or customer infrastructure
OS-Level Efficiency           CPU pinning, SO_REUSEPORT, GRO/GSO, systemd
CLI-first                     Fast Rust tool with SSE streaming


11. System Behavior Summary

âœ… Everything append-only: no mutation, full observability
âœ… Every operation idempotent and replay-safe
âœ… System scales horizontally by partition (run_id)
âœ… Agents modify DAG safely via validated overlays
âœ… Optimizer continuously improves runtime efficiency
âœ… Human approvals provide checkpoints for irreversible steps
âœ… Every artifact content-addressed for reproducibility
âœ… Full audit trail and replay capability


12. Agent Execution Environments

Flexible deployment â€” customer chooses:

Kubernetes Jobs (default)
â”œâ”€ Isolated pods per agent
â”œâ”€ Resource limits (CPU/memory)
â”œâ”€ Secrets via K8s secrets
â””â”€ Autoscaling via HPA

AWS Lambda (serverless)
â”œâ”€ Instant cold start
â”œâ”€ Pay-per-invocation
â”œâ”€ Auto-scaling
â””â”€ No infrastructure management

Customer Infrastructure (BYOE - Bring Your Own Environment)
â”œâ”€ On-prem K8s
â”œâ”€ Cloud Run / ECS / Fargate
â”œâ”€ Custom Docker deployments
â””â”€ Requirements:
    â€¢ Consume from Kafka/Redis queue
    â€¢ Publish results back
    â€¢ Network access to LLM APIs


13. What Makes This Unique

vs. Temporal/Airflow (Static Workflows)
âŒ They: Static DAGs, restart required
âœ… Us: Runtime patching, no restart

vs. AutoGPT/LangChain (Pure Agents)
âŒ They: No deterministic backbone, runaway costs
âœ… Us: Deterministic + agent overlays + spawn limits

vs. n8n (Low-Code)
âŒ They: UI-only, no agent patching, no CLI
âœ… Us: Agent patches + developer CLI + OS tuning

Unique Combination:
â€¢ Runtime workflow patching (safe, validated)
â€¢ Workflow-aware rate limiting (cost protection)
â€¢ Stateless coordinator (crash-resume)
â€¢ Customer execution environments (flexible)
â€¢ Fast CLI with SSE streaming (2.4MB Rust binary)
â€¢ OS-level optimization configs (production-ready)


14. MVP vs. Production

MVP (Phase 1 - Working)
â”œâ”€ Redis Streams (queues)
â”œâ”€ HTTP REST (communication)
â”œâ”€ JSON (serialization)
â”œâ”€ Local processes (agents)
â”œâ”€ Direct Postgres (metadata)
â””â”€ ~1,000 workflows/sec

Production (Phase 2+ - Designed)
â”œâ”€ Kafka/Redpanda (64-256 partitions)
â”œâ”€ gRPC (low-latency communication)
â”œâ”€ Protobuf (efficient serialization)
â”œâ”€ K8s/Lambda/customer env (agents)
â”œâ”€ Event sourcing + optional read models
â””â”€ ~10,000+ workflows/sec

All MVP decisions support migration â€” no throwaway code!


15. Demo Script (5â€“7 minutes)

1. Start run; watch timeline in UI
2. Agent analyzes data, proposes patch (add S3 upload + notifications)
3. Show overlay diff in UI (base + new nodes)
4. HITL approval before email (aob approve from CLI)
5. Optimizer detects sequential HTTP nodes â†’ fuses them
6. Success path: email sent; Fail path: Slack notification
7. Show CAS artifacts + cache hits
8. Replay from checkpoint: aob replay run_id --from parse


Appendix: Key Metrics

Current (MVP)
â”œâ”€ Throughput: 1,000 workflows/sec
â”œâ”€ Latency: 5-10ms per hop
â”œâ”€ Services: 6 microservices + CLI
â””â”€ Infrastructure: Redis + Postgres

Target (Production)
â”œâ”€ Throughput: 10,000+ workflows/sec
â”œâ”€ Latency: <2ms per hop
â”œâ”€ Services: Horizontally scaled
â””â”€ Infrastructure: Kafka + Postgres sharding + Dragonfly
